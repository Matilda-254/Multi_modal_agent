{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0a9dc65",
   "metadata": {},
   "source": [
    "### This is an AI agent that takes in both an image and a text then gives out a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "736053d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, Optional\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84381ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "groq_api_key = os.getenv(\"groq_api_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc245875",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    model_name=\"llama-3.3-70b-versatile\",\n",
    "    api_key=groq_api_key,\n",
    "    temperature=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5ce906b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BlipForConditionalGeneration(\n",
       "  (vision_model): BlipVisionModel(\n",
       "    (embeddings): BlipVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (encoder): BlipEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BlipEncoderLayer(\n",
       "          (self_attn): BlipAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): BlipMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (text_decoder): BlipTextLMHeadModel(\n",
       "    (bert): BlipTextModel(\n",
       "      (embeddings): BlipTextEmbeddings(\n",
       "        (word_embeddings): Embedding(30524, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (encoder): BlipTextEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BlipTextLayer(\n",
       "            (attention): BlipTextAttention(\n",
       "              (self): BlipTextSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): BlipTextSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BlipTextAttention(\n",
       "              (self): BlipTextSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): BlipTextSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BlipTextIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BlipTextOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): BlipTextOnlyMLMHead(\n",
       "      (predictions): BlipTextLMPredictionHead(\n",
       "        (transform): BlipTextPredictionHeadTransform(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (transform_act_fn): GELUActivation()\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): Linear(in_features=768, out_features=30524, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BLIP Captioning Model (for images -> text)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip-image-captioning-base\"\n",
    ").to(device)\n",
    "blip_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "503e5fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_image(image_path: str) -> str:\n",
    "    \"\"\"Generate a caption for the input image using BLIP.\"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    out_ids = blip_model.generate(**inputs, max_new_tokens=64)\n",
    "    caption = processor.decode(out_ids[0], skip_special_tokens=True)\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2a4e51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- LangGraph State Definition ----\n",
    "class AgentState(TypedDict):\n",
    "    question: str\n",
    "    image_path: Optional[str]\n",
    "    text_answer: Optional[str]\n",
    "    image_answer: Optional[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "065c254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Workflow ----\n",
    "workflow = StateGraph(AgentState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7f5c8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_only_node(state: AgentState) -> AgentState:\n",
    "    response = llm.invoke(state[\"question\"])\n",
    "    return {\"text_answer\": response.content}\n",
    "\n",
    "def image_qna_node(state: AgentState) -> AgentState:\n",
    "    caption = caption_image(state[\"image_path\"])\n",
    "    prompt = f\"Image description: {caption}\\nQuestion: {state['question']}\"\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"image_answer\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16332265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def text_only_node(state: AgentState) -> AgentState:\n",
    "#     prompt = ChatPromptTemplate.from_template(\n",
    "#         \"Answer the question: {question}\"\n",
    "#     )\n",
    "#     chain = prompt | llm\n",
    "#     response = chain.invoke({\"question\": state[\"question\"]})\n",
    "#     return {\"answer\": response.content}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf7bcc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def image_qna_node(state: AgentState) -> AgentState:\n",
    "#     caption = caption_image(state[\"image_path\"])\n",
    "#     prompt = ChatPromptTemplate.from_template(\n",
    "#         \"Image description: {caption}\\nUser question: {question}\\nAnswer concisely.\"\n",
    "#     )\n",
    "#     chain = prompt | llm\n",
    "#     response = chain.invoke({\"caption\": caption, \"question\": state[\"question\"]})\n",
    "#     return {\"answer\": response.content} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ddf0240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x23bd92849d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add nodes\n",
    "workflow.add_node(\"text_only\", text_only_node)\n",
    "workflow.add_node(\"image_qna\", image_qna_node)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcc6dbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Router logic (pure function for edges, not a node) ---\n",
    "def router(state: AgentState) -> str:\n",
    "    if state.get(\"image_path\"):\n",
    "        return \"image_qna\"\n",
    "    return \"text_only\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c433f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x23bd92849d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Entry point\n",
    "workflow.set_entry_point(\"text_only\")  # dummy, will be replaced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a489252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x23bd92849d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Use START -> router conditional edges\n",
    "workflow.add_conditional_edges(\n",
    "    START,   # from graph entrypoint\n",
    "    router,  # function returning a string\n",
    "    {\n",
    "        \"text_only\": \"text_only\",\n",
    "        \"image_qna\": \"image_qna\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# End edges\n",
    "workflow.add_edge(\"text_only\", END)\n",
    "workflow.add_edge(\"image_qna\", END)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9d8145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compile\n",
    "app = workflow.compile()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10b2b66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Answer: In deep learning, an activation key, more commonly referred to as an activation function, is a mathematical function that is applied to the output of a neural network layer. The primary purpose of an activation function is to introduce non-linearity into the model, allowing it to learn and represent more complex relationships between the inputs and outputs.\n",
      "\n",
      "Without activation functions, neural networks would only be able to learn linear relationships, which would limit their ability to model complex data. The activation function helps to:\n",
      "\n",
      "1. **Introduce non-linearity**: By applying a non-linear transformation to the output of a layer, the model can learn to represent more complex relationships between the inputs and outputs.\n",
      "2. **Control the output range**: Activation functions can help to control the range of the output values, which can be useful for certain types of problems, such as binary classification.\n",
      "3. **Help with vanishing gradients**: Some activation functions, such as ReLU (Rectified Linear Unit), can help to mitigate the vanishing gradient problem, which can occur when training deep neural networks.\n",
      "\n",
      "Common examples of activation functions include:\n",
      "\n",
      "1. **Sigmoid**: Maps the input to a value between 0 and 1, often used for binary classification problems.\n",
      "2. **ReLU (Rectified Linear Unit)**: Maps all negative values to 0 and all positive values to the same value, often used for hidden layers.\n",
      "3. **Tanh (Hyperbolic Tangent)**: Maps the input to a value between -1 and 1, often used for hidden layers.\n",
      "4. **Softmax**: Maps the input to a probability distribution, often used for multi-class classification problems.\n",
      "\n",
      "In summary, an activation key or activation function is a crucial component of deep learning models, allowing them to learn and represent complex relationships between inputs and outputs.\n",
      "Image Answer: Based on the image description of the Forex trading app on an iPhone, I'll provide an interpretation of the graph.\n",
      "\n",
      "The graph on the Forex trading app likely displays a currency pair's price movement over a specific period. Here's a breakdown of what the graph might show:\n",
      "\n",
      "1. **Currency Pair**: The graph probably displays the price movement of a specific currency pair, such as EUR/USD, USD/JPY, or GBP/USD.\n",
      "2. **Time Frame**: The graph might show the price movement over a short-term period, such as 1 minute, 5 minutes, 1 hour, or 4 hours, or a longer-term period, like daily, weekly, or monthly.\n",
      "3. **Price Axis**: The vertical axis (y-axis) represents the price of the currency pair, while the horizontal axis (x-axis) represents the time.\n",
      "4. **Candlesticks or Lines**: The graph might display candlesticks or lines that represent the price movement. Candlesticks typically show the high, low, open, and close prices for a specific time period, while lines might show the closing prices over time.\n",
      "5. **Trends and Patterns**: The graph might display trends, such as uptrends, downtrends, or sideways movements, as well as patterns like support and resistance levels, chart patterns (e.g., head and shoulders, triangles), or indicators (e.g., moving averages, RSI).\n",
      "\n",
      "Some possible interpretations of the graph include:\n",
      "\n",
      "* **Uptrend**: If the graph shows a series of higher highs and higher lows, it might indicate an uptrend, suggesting that the currency pair's price is increasing.\n",
      "* **Downtrend**: Conversely, if the graph shows a series of lower highs and lower lows, it might indicate a downtrend, suggesting that the currency pair's price is decreasing.\n",
      "* **Consolidation**: If the graph shows a sideways movement, it might indicate a period of consolidation, where the currency pair's price is ranging between support and resistance levels.\n",
      "\n",
      "Keep in mind that this interpretation is based on a general understanding of Forex trading graphs and might not be specific to the actual graph displayed on the iPhone. If you have more information about the graph or the context in which it's being used, I'd be happy to provide a more detailed interpretation.\n"
     ]
    }
   ],
   "source": [
    "# ---- Test the usage ----\n",
    "if __name__ == \"__main__\":\n",
    "    # Text question\n",
    "    result1 = app.invoke({\"question\": \"What is an activation key in deep learning?\"})\n",
    "    print(\"Text Answer:\", result1.get(\"text_answer\"))\n",
    "\n",
    "    # Image-related Q&A\n",
    "    result2 = app.invoke({\n",
    "        \"question\": \"Interpret the Image of a graph given\",\n",
    "        \"image_path\": \"bnb.jpg\"\n",
    "    })\n",
    "    print(\"Image Answer:\", result2.get(\"image_answer\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
